{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceso finalizado. Se procesaron 25650 nodos y se eliminaron campos de country.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 1. Ruta de entrada/salida: ajusta segÃºn corresponda\n",
    "INPUT_PATH  = \"/home/lenovo/Documentos/repos/final-project-cinedive/static/data/graph_for_project.json\"\n",
    "OUTPUT_PATH = \"/home/lenovo/Documentos/repos/final-project-cinedive/static/data/graph_for_project_sin_country.json\"\n",
    "\n",
    "def procesar_json(input_path, output_path):\n",
    "    # 1. Leer JSON original\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        grafo = json.load(f)\n",
    "\n",
    "    nodos = grafo.get(\"nodes\", [])\n",
    "\n",
    "    # 2. Eliminar cualquier campo relacionado con \"country\" en cada nodo\n",
    "    for nodo in nodos:\n",
    "        if \"country_origin\" in nodo:\n",
    "            nodo.pop(\"country_origin\", None)\n",
    "        # Si hubiera otros campos que incluyan la palabra \"country\", se pueden eliminar asÃ­:\n",
    "        # for clave in list(nodo.keys()):\n",
    "        #     if \"country\" in clave:\n",
    "        #         nodo.pop(clave, None)\n",
    "\n",
    "    # 3. Guardar nuevo JSON sin campos de paÃ­s\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(grafo, f_out, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Proceso finalizado. Se procesaron {len(nodos)} nodos y se eliminaron campos de country.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    procesar_json(INPUT_PATH, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_list_field(raw):\n",
    "    \"\"\"\n",
    "    Garante que o resultado seja uma lista, mesmo se for uma string simples.\n",
    "    \"\"\"\n",
    "    raw = raw.strip()\n",
    "    if not raw:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        value = ast.literal_eval(raw)\n",
    "        if isinstance(value, list):\n",
    "            return value\n",
    "        else:\n",
    "            return [str(value)]\n",
    "    except (ValueError, SyntaxError):\n",
    "        return [raw]\n",
    "\n",
    "def parse_person_field(field):\n",
    "    people = []\n",
    "    if not field.strip():\n",
    "        return people\n",
    "\n",
    "    raw_items = field.strip()[1:-1].split('),(')\n",
    "    for item in raw_items:\n",
    "        item = item.strip('()')\n",
    "        try:\n",
    "            parts = item.split(',', 2)\n",
    "            if len(parts) == 3:\n",
    "                pid = parts[0].strip()\n",
    "                role_type = parts[1].strip()\n",
    "                name_part = parts[2].strip()\n",
    "                if name_part == r'\\N':\n",
    "                    continue  # pula registros sem nome\n",
    "                name = ast.literal_eval(name_part)[0]\n",
    "                people.append({\n",
    "                    \"id\": pid,\n",
    "                    \"name\": name,\n",
    "                    \"type\": role_type\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar pessoa: {item} -> {e}\")\n",
    "            continue\n",
    "    return people\n",
    "\n",
    "def process_tsv_to_json(tsv_file, output_json):\n",
    "    nodes = {}\n",
    "    links_dict = defaultdict(lambda: {\"filmes\": [], \"year\": None, \"country\": None})\n",
    "    \n",
    "    with open(tsv_file, encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            movie = row['primaryTitle'].strip()\n",
    "            year = row['startYear'].strip()\n",
    "\n",
    "            country_list = parse_list_field(row.get('country_origin', ''))\n",
    "            country = country_list[0] if country_list else None\n",
    "\n",
    "            directors = [d.strip() for d in row.get('directors', '').split(',') if d.strip()]\n",
    "            writers = [w.strip() for w in row.get('writers', '').split(',') if w.strip()]\n",
    "            people = parse_person_field(row.get('person', ''))\n",
    "\n",
    "            for d in directors:\n",
    "                if d not in nodes:\n",
    "                    nodes[d] = {\"id\": d, \"name\": d, \"type\": \"director\"}\n",
    "            for w in writers:\n",
    "                if w not in nodes:\n",
    "                    nodes[w] = {\"id\": w, \"name\": w, \"type\": \"writer\"}\n",
    "            for p in people:\n",
    "                if p['id'] not in nodes:\n",
    "                    nodes[p['id']] = {\n",
    "                        \"id\": p['id'],\n",
    "                        \"name\": p['name'],\n",
    "                        \"type\": p['type']\n",
    "                    }\n",
    "\n",
    "            all_ids = directors + writers + [p['id'] for p in people]\n",
    "            for i in range(len(all_ids)):\n",
    "                for j in range(i+1, len(all_ids)):\n",
    "                    source, target = sorted((all_ids[i], all_ids[j]))\n",
    "                    key = (source, target)\n",
    "                    links_dict[key][\"filmes\"].append(movie)\n",
    "                    links_dict[key][\"year\"] = year\n",
    "                    links_dict[key][\"country\"] = country\n",
    "\n",
    "    nodes_list = list(nodes.values())\n",
    "    links_list = [\n",
    "        {\n",
    "            \"source\": source,\n",
    "            \"target\": target,\n",
    "            \"filmes\": data[\"filmes\"],\n",
    "            \"year\": data[\"year\"],\n",
    "            \"country\": data[\"country\"]\n",
    "        }\n",
    "        for (source, target), data in links_dict.items()\n",
    "    ]\n",
    "\n",
    "    result = {\n",
    "        \"nodes\": nodes_list,\n",
    "        \"links\": links_list\n",
    "    }\n",
    "\n",
    "    with open(output_json, 'w', encoding='utf-8') as f_out:\n",
    "        json.dump(result, f_out, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ðŸ”§ Exemplo de uso:\n",
    "process_tsv_to_json(\"../static/data/title_oscar_con_country.tsv\", \"graphic_person.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tacd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
